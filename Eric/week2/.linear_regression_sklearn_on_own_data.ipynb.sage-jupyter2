{"backend_state":"running","connection_file":"/projects/200e8042-15d9-45b5-b665-c79cbc8761e9/.local/share/jupyter/runtime/kernel-f58ed5b8-8fd6-403c-96c7-00003a4d0d18.json","kernel":"python3-ubuntu","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"colab":{"collapsed_sections":[],"name":"linear_regression_sklearn_on_own_data.ipynb","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"trust":true,"type":"settings"}
{"cell_type":"code","end":1658943783129,"exec_count":5,"id":"4f7789","input":"my_data['smoker']","kernel":"python3-ubuntu","output":{"0":{"data":{"text/plain":"0       yes\n1        no\n2        no\n3        no\n4        no\n       ... \n1333     no\n1334     no\n1335     no\n1336     no\n1337    yes\nName: smoker, Length: 1338, dtype: object"},"exec_count":5}},"pos":6,"start":1658943783095,"state":"done","type":"cell"}
{"cell_type":"code","end":1658943804946,"exec_count":10,"id":"fec931","input":"sns.heatmap(my_data.corr())","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"H7FyJo_QutAX","outputId":"d1ad6159-df64-4882-e2ae-d78cfa814cc5"},"output":{"0":{"data":{"text/plain":"<matplotlib.axes._subplots.AxesSubplot at 0x7f64087730d0>"},"exec_count":10},"1":{"data":{"image/png":"f85e5a980b847aa05ebb19e3bcb7fbceb88c5577","text/plain":"<Figure size 864x504 with 2 Axes>"},"metadata":{"image/png":{"height":415,"width":641},"needs_background":"light"}}},"pos":13,"start":1658943804149,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946071742,"exec_count":30,"id":"e955ea","input":"# import libraries \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport statistics","kernel":"python3-ubuntu","metadata":{"id":"IecuRdF1a-sG"},"pos":1,"start":1658946071676,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946076581,"exec_count":31,"id":"50110d","input":"filename = 'https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv'\nmy_data = pd.read_csv(filename) #TODO: read in your file by replacing the filename variable with your file's path. You can also use this current code to work on an automobile dataset! ","kernel":"python3-ubuntu","metadata":{"id":"mBcweiAXW3bC"},"pos":5,"start":1658946075221,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946080202,"exec_count":32,"id":"f7fc78","input":"#cleaning the data -- dropping missing and duplicate values for sanity\nmy_data.dropna(inplace = True)\nmy_data.drop_duplicates(inplace = True)\nmy_data = my_data.reset_index(drop=True)\n\nlength = len(my_data.index) #save length of array of later","kernel":"python3-ubuntu","metadata":{"id":"BYHvay0xa-sK"},"pos":8,"start":1658946080147,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946082741,"exec_count":33,"id":"5f1ff0","input":"def change_smoke(val):\n    if val == 'no':\n        return 0\n    elif val == 'yes':\n        return 1\nmy_data['smoker'] = my_data['smoker'].apply(change_smoke)\nmy_data['smoker']","kernel":"python3-ubuntu","output":{"0":{"data":{"text/plain":"0       1\n1       0\n2       0\n3       0\n4       0\n       ..\n1332    0\n1333    0\n1334    0\n1335    0\n1336    1\nName: smoker, Length: 1337, dtype: int64"},"exec_count":33}},"pos":9,"start":1658946082707,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946085691,"exec_count":34,"id":"bccea4","input":"def change(val):\n    if val == 'southwest':\n        return 0\n    elif val == 'southeast':\n        return 1\n    elif val == 'northwest':\n        return 2\n    elif val == 'northeast':\n        return 3\nmy_data['region'] = my_data['region'].apply(change)\nmy_data['region']","kernel":"python3-ubuntu","output":{"0":{"data":{"text/plain":"0       0\n1       1\n2       1\n3       2\n4       2\n       ..\n1332    2\n1333    3\n1334    1\n1335    0\n1336    2\nName: region, Length: 1337, dtype: int64"},"exec_count":34}},"pos":10,"start":1658946085653,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946087294,"exec_count":35,"id":"6d425f","input":"def change_sex(val):\n    if val == 'female':\n        return 0\n    elif val=='male':\n        return 1\nmy_data['sex'] = my_data['sex'].apply(change_sex)\nmy_data['sex']","kernel":"python3-ubuntu","output":{"0":{"data":{"text/plain":"0       0\n1       1\n2       1\n3       1\n4       1\n       ..\n1332    1\n1333    0\n1334    0\n1335    0\n1336    0\nName: sex, Length: 1337, dtype: int64"},"exec_count":35}},"pos":11,"start":1658946087277,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946091198,"exec_count":36,"id":"71df4f","input":"X = my_data.drop('charges',axis=1)\nX = X.to_numpy()\ny = my_data[\"charges\"].to_numpy()\nX","kernel":"python3-ubuntu","metadata":{"id":"WW77IzOwZaKp"},"output":{"0":{"data":{"text/plain":"array([[19.  ,  0.  , 27.9 ,  0.  ,  1.  ,  0.  ],\n       [18.  ,  1.  , 33.77,  1.  ,  0.  ,  1.  ],\n       [28.  ,  1.  , 33.  ,  3.  ,  0.  ,  1.  ],\n       ...,\n       [18.  ,  0.  , 36.85,  0.  ,  0.  ,  1.  ],\n       [21.  ,  0.  , 25.8 ,  0.  ,  0.  ,  0.  ],\n       [61.  ,  0.  , 29.07,  0.  ,  1.  ,  2.  ]])"},"exec_count":36}},"pos":15,"start":1658946091161,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946115048,"exec_count":38,"id":"d1ca07","input":"# Predicting using SKLearn\ny_hat = linr.predict(x_test)","kernel":"python3-ubuntu","metadata":{"id":"o9ZDJB1qrJ7_"},"pos":24,"start":1658946115040,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946127524,"exec_count":40,"id":"6df3b2","input":"print(linr.score(x_test, y_test))","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CS2KN6gms_io","outputId":"4f865d48-e1ea-4e80-ebb1-869f5f2052b0"},"output":{"0":{"name":"stdout","text":"0.806846632262911\n"}},"pos":28,"start":1658946127472,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946129278,"exec_count":41,"id":"4de3f1","input":"MAE = np.mean(abs(y_test - y_hat))\nMAE","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osxC5vVotKWS","outputId":"aa7e2d67-12af-4389-e7e0-4c23b23e8bf4"},"output":{"0":{"data":{"text/plain":"4182.3531552883005"},"exec_count":41}},"pos":29,"start":1658946129268,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946255199,"exec_count":58,"id":"1db22b","input":"from sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split\n# splitting the data\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n#Create the model object\nlinr = LinearRegression()\n#Fit (train) the model -- this is where the ML happens!\nlinr.fit(x_train, y_train)\nprint(linr.intercept_, linr.coef_[0])\npoly = PolynomialRegression(3)\npoly.fit(x_train,y_train)","kernel":"python3-ubuntu","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkPf0bt4a-sV","outputId":"99433807-734f-4a07-fe0a-26b3ad2fb709"},"output":{"0":{"name":"stdout","text":"-11760.561999173467 248.76407133644213\n"},"1":{"data":{"text/plain":"Pipeline(steps=[('polynomialfeatures', PolynomialFeatures(degree=3)),\n                ('linearregression', LinearRegression())])"},"exec_count":58}},"pos":22,"start":1658946255136,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946260818,"exec_count":59,"id":"1e5d80","input":"y_hat2 = poly.predict(x_test)","kernel":"python3-ubuntu","pos":25,"start":1658946260787,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946263260,"exec_count":60,"id":"e42e30","input":"print(poly.score(x_test,y_test))","kernel":"python3-ubuntu","output":{"0":{"name":"stdout","text":"0.877253993754613\n"}},"pos":30,"start":1658946263247,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946264924,"exec_count":61,"id":"30c839","input":"MAE = np.mean(abs(y_test - y_hat2))\nMAE","kernel":"python3-ubuntu","output":{"0":{"data":{"text/plain":"3012.135518512749"},"exec_count":61}},"pos":31,"start":1658946264913,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946270278,"exec_count":62,"id":"ce0cfd","input":"import plotly.express as px\npx.scatter(x=y_hat,y=y_hat2)","kernel":"python3-ubuntu","output":{"0":{"data":{"iframe":"b3bc3868154e9bfe57f8359506af919888fb1439"}}},"pos":35,"start":1658946270144,"state":"done","type":"cell"}
{"cell_type":"code","end":1658946273637,"exec_count":63,"id":"612438","input":"import plotly.express as px\npx.scatter(x=y_hat2,y=y_test)","kernel":"python3-ubuntu","output":{"0":{"data":{"iframe":"f39879d5c4b44e029e03aa54012d6ce911971740"}}},"pos":34,"start":1658946273448,"state":"done","type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c2da93","input":"","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":26,"id":"8672d4","input":"# Checking for Missing Data\nif X.shape[0] != y.shape[0]:\n  print(\"It looks like you have missing data. You may want to preprocess your data more with pandas to delete any rows with missing, NaN, N/A, and null values.\")\n  \nidx = np.arange(length) #shuffle our dataset indices so we don't always split the same way!\nnp.random.shuffle(idx)\n\n#split our data with 80% for training (learning) and 20% for testing.\nsplit_threshold = int(length * 0.8)\n\ntrain_idx = idx[:split_threshold]\n# Uses the remaining indices for testing\ntest_idx = idx[split_threshold:]\n\n# Generates train and test sets and formats them for training.\nx_train, y_train = X[train_idx], y[train_idx]\nx_test, y_test = X[test_idx], y[test_idx]\nx_train= x_train.reshape(-1, 1)\ny_train= y_train.reshape(-1, 1)\nx_test = x_test.reshape(-1, 1)","metadata":{"id":"wKS4pvcEsLOF"},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"1ff6d5","input":"#let's plot our split data to see how it looks!\nplt.figure(figsize=(10,5))\n\n# plot the train set \nplt.subplot(1,2,1)\nplt.scatter(x_train,y_train, c='orange')  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Generated Data - Train')\nplt.grid('on')\n\n# plot the test set \nplt.subplot(1,2,2)\nplt.scatter(x_test, y_test)  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Generated Data - Test')\nplt.grid('on')\n\nplt.show()","metadata":{"id":"6IxwdW5osUDI"},"output":{"0":{"ename":"ValueError","evalue":"x and y must be the same size","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-54c9ff6530d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plot the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orange'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2834\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 2836\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   2837\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2838\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4441\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4443\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must be the same size"]},"1":{"data":{"image/png":"0476fd12729a83cadfc75bc7f278ea0391dd8f42","text/plain":"<Figure size 720x360 with 1 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":306,"width":298},"needs_background":"light"},"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":33,"id":"ea963a","input":"#plotting results\nplt.figure(figsize=(10,5))\nplt.plot(x_test, y_hat, '--')\n\nplt.scatter(x_test,y_test, c='orange')  \nplt.xlabel('x', fontsize = 20) \nplt.ylabel('y', fontsize = 20)\nplt.title('Generated Data - Test')\nplt.grid('on')\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"id":"RarVj0Kor54b","outputId":"c3649dc9-e5f0-4930-f4b1-100ad0987777"},"output":{"0":{"ename":"ValueError","evalue":"x and y must be the same size","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-2e3040016c56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orange'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2834\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m         edgecolors=None, *, plotnonfinite=False, data=None, **kwargs):\n\u001b[0;32m-> 2836\u001b[0;31m     __ret = gca().scatter(\n\u001b[0m\u001b[1;32m   2837\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2838\u001b[0m         \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4441\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4443\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4445\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: x and y must be the same size"]},"1":{"data":{"image/png":"8a0fe3fffaf19f3368857abf21eb174044d5a927","text/plain":"<Figure size 720x360 with 1 Axes>"},"exec_count":33,"metadata":{"image/png":{"height":302,"width":611},"needs_background":"light"},"output_type":"execute_result"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":39,"id":"fc77ef","input":"y_test","output":{"0":{"data":{"text/plain":"array([ 8688.85885 ,  5708.867   , 11436.73815 , 38746.3551  ,\n        4463.2051  ,  9304.7019  , 38511.6283  ,  2150.469   ,\n        7345.7266  , 10264.4421  , 13415.0381  , 24393.6224  ,\n       37607.5277  , 13063.883   , 24915.04626 ,  8027.968   ,\n        1980.07    , 38709.176   ,  3484.331   ,  3947.4131  ,\n        1880.07    , 20773.62775 ,  9800.8882  , 21082.16    ,\n       55135.40209 ,  3579.8287  , 43896.3763  , 52590.82939 ,\n        9778.3472  , 10736.87075 ,  4347.02335 ,  9048.0273  ,\n        1711.0268  , 12333.828   , 63770.42801 ,  9872.701   ,\n        4687.797   ,  5693.4305  , 21195.818   ,  8233.0975  ,\n        2755.02095 , 19798.05455 , 42124.5153  ,  9964.06    ,\n        9193.8385  ,  3161.454   ,  2217.6012  ,  7337.748   ,\n        4133.64165 ,  6338.0756  ,  4349.462   ,  5757.41345 ,\n       21771.3423  ,  1391.5287  ,  7160.094   ,  7731.4271  ,\n        9957.7216  ,  3594.17085 , 38282.7495  ,  7256.7231  ,\n       10977.2063  ,  5245.2269  , 11187.6567  ,  1708.92575 ,\n       11411.685   , 11729.6795  ,  6079.6715  , 37829.7242  ,\n       16297.846   , 31620.00106 ,  9991.03765 ,  2203.47185 ,\n        6858.4796  ,  3167.45585 , 10796.35025 ,  1728.897   ,\n       12574.049   ,  4889.9995  ,  9182.17    , 30259.99556 ,\n       16796.41194 , 12592.5345  , 12741.16745 , 18806.14547 ,\n       29523.1656  , 12925.886   ,  3645.0894  ,  1719.4363  ,\n        1984.4533  ,  1824.2854  , 13919.8229  , 18765.87545 ,\n        5615.369   ,  1526.312   ,  1743.214   , 43254.41795 ,\n        9504.3103  ,  8232.6388  ,  1674.6323  ,  8604.48365 ,\n       13880.949   , 12363.547   , 23244.7902  , 11264.541   ,\n       39983.42595 ,  1627.28245 , 37701.8768  , 17942.106   ,\n       12269.68865 ,  4746.344   ,  6313.759   , 12222.8983  ,\n        5594.8455  ,  6551.7501  ,  1682.597   ,  1986.9334  ,\n        2710.82855 ,  1135.9407  ,  1837.2819  , 10107.2206  ,\n       44423.803   , 12404.8791  ,  7152.6714  ,  5934.3798  ,\n       28340.18885 ,  6875.961   ,  5080.096   , 13041.921   ,\n       38126.2465  , 48824.45    , 29141.3603  ,  7935.29115 ,\n        2137.6536  , 11085.5868  ,  5972.378   ,  4561.1885  ,\n       42983.4585  , 19594.80965 , 17748.5062  ,  2473.3341  ,\n        5152.134   ,  4564.19145 , 45863.205   , 25517.11363 ,\n        1146.7966  ,  5649.715   , 11326.71487 ,  7626.993   ,\n       42969.8527  , 13555.0049  ,  6571.544   ,  6985.50695 ,\n        4751.07    , 14319.031   ,  6593.5083  , 34439.8559  ,\n       46113.511   ,  9432.9253  ,  8280.6227  ,  8835.26495 ,\n       10923.9332  ,  5458.04645 , 11842.62375 , 12231.6136  ,\n       43578.9394  , 39556.4945  , 10560.4917  , 17085.2676  ,\n        3206.49135 ,  3861.20965 ,  4239.89265 ,  1633.0444  ,\n        4137.5227  , 11833.7823  ,  8310.83915 , 32734.1863  ,\n        2597.779   ,  8283.6807  ,  8428.0693  , 10141.1362  ,\n       14119.62    ,  2855.43755 , 25992.82104 , 11830.6072  ,\n       40003.33225 , 10594.50155 ,  3597.596   ,  3591.48    ,\n        2117.33885 , 38711.      , 17878.90068 , 23887.6627  ,\n       10564.8845  ,  4562.8421  ,  2198.18985 ,  1131.5066  ,\n       39125.33225 , 20177.67113 , 14001.2867  , 15230.32405 ,\n       33750.2918  , 16577.7795  ,  4889.0368  , 20781.48892 ,\n       19933.458   ,  5012.471   ,  7749.1564  ,  7441.501   ,\n        2690.1138  , 41949.2441  , 22478.6     ,  9301.89355 ,\n        4779.6023  ,  7201.70085 , 12629.8967  , 14133.03775 ,\n        2404.7338  ,  6948.7008  ,  8827.2099  ,  2203.73595 ,\n        3213.62205 , 46151.1245  , 46200.9851  , 14988.432   ,\n        9377.9047  , 46889.2612  , 41097.16175 ,  9644.2525  ,\n       34617.84065 ,  4032.2407  , 12981.3457  , 35491.64    ,\n        3877.30425 ,  5979.731   , 11534.87265 ,  8277.523   ,\n       13204.28565 ,  1629.8335  ,  3021.80915 ,  3392.9768  ,\n        3443.064   ,  2699.56835 , 20277.80751 ,  8932.084   ,\n        1632.03625 ,  9566.9909  ,  4391.652   ,  7196.867   ,\n        7323.734819,  8410.04685 ,  2719.27975 , 11658.11505 ,\n       12146.971   , 13607.36875 , 60021.39897 , 13747.87235 ,\n       12029.2867  ,  6113.23105 ,  8059.6791  , 25333.33284 ,\n       37465.34375 , 47055.5321  , 12949.1554  , 13831.1152  ,\n       13887.204   ,  3925.7582  , 47403.88    ,  8534.6718  ])"},"exec_count":39,"output_type":"execute_result"}},"pos":32,"scrolled":true,"type":"cell"}
{"cell_type":"code","exec_count":42,"id":"94541f","input":"y_hat","output":{"0":{"data":{"text/plain":"array([ 8.08045019e+03,  5.59287112e+03,  1.43782987e+04,  3.17318195e+04,\n        9.15835694e+03,  1.33610143e+04,  3.02576553e+04,  1.30864736e+03,\n        1.08494254e+04,  1.13755485e+04,  1.04822214e+04,  3.31558608e+04,\n        3.09123841e+04,  1.71688443e+04,  1.06361393e+04,  9.37424715e+03,\n        3.96524338e+03,  3.19229863e+04,  3.08468457e+03,  5.47544373e+03,\n        3.76844305e+03,  3.01552667e+04,  1.50768984e+04,  3.02746510e+04,\n        3.10830548e+04,  5.52001772e+03,  3.56655758e+04,  3.63713163e+04,\n        1.12979802e+04,  1.40664742e+04,  6.37519963e+03,  1.29381095e+04,\n        6.79284837e+02,  1.19476924e+04,  3.97832706e+04,  1.21517558e+04,\n        4.57717842e+03,  4.01652351e+03,  3.11475453e+04,  9.22301501e+03,\n        6.86852604e+03,  3.00358113e+04,  3.49732855e+04,  1.21709787e+04,\n        7.46262777e+03,  3.27014535e+03,  6.02195905e+03,  8.81742438e+03,\n        4.32171874e+03,  9.25718479e+03,  6.82679737e+03,  1.18887556e+04,\n        3.11350727e+04,  3.77190864e+03,  1.08474792e+04,  9.98520023e+03,\n        1.07454815e+04,  2.50630360e+03,  3.13486291e+04,  9.44527226e+03,\n        1.57668832e+04,  8.41228194e+03,  1.26377467e+04,  1.49728263e+03,\n        1.67511566e+04,  1.06295158e+04,  1.04219387e+04,  3.05792819e+04,\n        2.45776833e+04,  1.64325761e+04,  7.38210684e+03,  2.84428811e+03,\n        7.06992638e+03, -3.29990956e+01,  1.15924890e+04, -1.21951641e+03,\n        1.68815835e+04,  8.83059721e+03,  7.62157102e+03,  1.27175180e+04,\n        5.76577492e+03,  1.86978721e+04,  1.53309296e+04,  1.11806899e+04,\n        3.63483776e+04,  1.34071783e+04,  4.27456647e+03,  2.57056956e+03,\n        6.47918751e+03,  4.27749189e+03,  1.52560601e+04,  2.82268928e+04,\n        5.37214077e+03,  3.08592939e+03,  2.00035675e+03,  3.57507981e+04,\n        1.45257606e+04,  9.11985403e+03,  4.40698476e+03,  8.62880140e+03,\n        1.38525421e+04,  1.49580797e+04,  3.24003275e+04,  1.43680921e+04,\n        3.27374776e+04,  1.42320024e+02,  3.09750462e+04,  2.65864768e+04,\n        1.39482983e+04,  5.85231109e+03,  8.37793412e+03,  1.16365922e+04,\n        8.33866348e+03,  1.01084727e+04,  5.96060944e+03,  5.55786581e+03,\n        2.02344638e+03,  3.27438050e+03,  7.20038646e+03,  9.34232538e+03,\n        3.62150782e+04,  8.07973818e+03,  1.17550080e+04,  9.41521529e+03,\n        9.08526942e+03,  9.47332252e+03,  4.50080494e+03,  1.22296214e+04,\n        2.99511114e+04,  3.87490882e+04,  3.60289943e+04,  1.21149339e+04,\n        2.64302635e+03,  1.15458208e+04,  1.02115068e+04,  5.36684378e+03,\n        3.48885825e+04,  2.98098502e+04,  2.55215076e+04,  5.07298229e+03,\n        6.58387957e+03,  5.66505956e+03,  3.66686088e+04,  1.07790251e+04,\n        5.71585714e+03,  8.19481345e+03,  1.03940830e+04,  8.31592675e+03,\n        3.52061174e+04,  1.43138669e+04,  9.63950922e+03,  6.75703968e+03,\n        6.91518184e+03,  1.65709176e+04,  7.20202021e+03,  2.66062187e+04,\n        3.64442937e+04,  1.64908193e+04,  1.01492581e+04,  1.11207708e+04,\n        1.26016369e+04,  4.11467259e+03,  1.28430232e+04,  1.35966509e+04,\n        3.56872909e+04,  3.31848044e+04,  1.30859575e+04,  2.54290450e+04,\n        2.50678646e+03,  2.00467020e+03,  4.96831556e+03,  5.19658662e+03,\n        5.14640620e+03,  1.00234872e+04,  1.00721937e+04,  2.63490328e+04,\n        2.25048316e+03,  1.08369980e+04,  6.95878414e+03,  1.20177625e+04,\n        1.49551151e+04,  5.10897460e+03,  1.11683409e+04,  1.01405180e+04,\n        3.30314071e+04,  1.09390939e+04,  7.06147444e+03,  5.68599464e+03,\n        4.79598292e+02,  3.13195316e+04,  8.08998400e+03,  3.27799499e+04,\n        1.25310600e+04,  6.22173594e+03,  1.65637374e+03,  2.27715765e+03,\n        3.26352160e+04,  4.86982248e+03,  1.34564157e+04,  1.73527692e+04,\n        2.63721437e+04,  2.59829197e+04,  6.78583233e+03,  1.23754902e+04,\n        2.82978484e+04,  7.76646211e+03,  1.03171120e+04,  1.00958943e+04,\n        3.22682827e+03,  3.44859791e+04,  3.14033491e+04,  9.70862267e+03,\n        6.05785607e+03,  7.71103584e+03,  1.10712073e+04,  4.92149066e+03,\n        5.10512887e+03,  1.19149215e+04,  9.30920134e+03,  2.90368383e+03,\n        4.11047086e+03,  3.77140235e+04,  3.72634878e+04,  1.31316765e+04,\n        1.09861059e+04,  3.79416155e+04,  3.42917572e+04,  7.04424919e+03,\n        2.68658684e+04,  4.15529217e+03,  1.59393494e+04,  2.92782324e+04,\n        5.80290476e+03,  9.24780203e+03,  8.76385695e+03,  9.21451482e+03,\n        9.60984456e+03,  4.47445973e+03,  5.71411299e+03,  4.83231250e+03,\n        6.09370956e+03,  4.79405873e+03,  5.86662908e+03,  8.68043311e+03,\n        1.21144296e+03,  8.50633917e+03,  3.65730220e+03,  1.07277674e+04,\n        2.54730952e+03,  1.03284320e+04,  3.92410938e+03,  1.18221571e+04,\n        1.20999883e+04,  1.43976542e+04,  3.69855629e+04,  2.31239381e+04,\n        1.00811646e+04,  7.79937510e+03,  9.23636727e+03,  1.23969398e+04,\n        2.93814869e+04,  3.80711084e+04,  1.49670597e+04,  1.69526824e+04,\n        1.54969080e+04,  6.30338553e+03,  3.82483329e+04,  8.24687676e+03])"},"exec_count":42,"output_type":"execute_result"}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"df69d3","input":"my_data.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"2n8n7O_8cUxX","outputId":"b1d56a4f-9c74-4cf0-b1eb-055ea2aab14e"},"output":{"0":{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>sex</th>\n      <th>bmi</th>\n      <th>children</th>\n      <th>smoker</th>\n      <th>region</th>\n      <th>charges</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>female</td>\n      <td>27.900</td>\n      <td>0</td>\n      <td>yes</td>\n      <td>southwest</td>\n      <td>16884.92400</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>18</td>\n      <td>male</td>\n      <td>33.770</td>\n      <td>1</td>\n      <td>no</td>\n      <td>southeast</td>\n      <td>1725.55230</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28</td>\n      <td>male</td>\n      <td>33.000</td>\n      <td>3</td>\n      <td>no</td>\n      <td>southeast</td>\n      <td>4449.46200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>33</td>\n      <td>male</td>\n      <td>22.705</td>\n      <td>0</td>\n      <td>no</td>\n      <td>northwest</td>\n      <td>21984.47061</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>32</td>\n      <td>male</td>\n      <td>28.880</td>\n      <td>0</td>\n      <td>no</td>\n      <td>northwest</td>\n      <td>3866.85520</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   age     sex     bmi  children smoker     region      charges\n0   19  female  27.900         0    yes  southwest  16884.92400\n1   18    male  33.770         1     no  southeast   1725.55230\n2   28    male  33.000         3     no  southeast   4449.46200\n3   33    male  22.705         0     no  northwest  21984.47061\n4   32    male  28.880         0     no  northwest   3866.85520"},"exec_count":5,"output_type":"execute_result"}},"pos":7,"type":"cell"}
{"cell_type":"markdown","id":"0d6d83","input":"# Data Loading, Cleaning, and Setup","metadata":{"id":"JlrQHpg8a-sI"},"pos":2,"type":"cell"}
{"cell_type":"markdown","id":"15d948","input":"### Creating and Fitting\n\n","metadata":{"id":"e1m-wKeYsFEl"},"pos":21,"type":"cell"}
{"cell_type":"markdown","id":"2a5a4a","input":"np docs - https://numpy.org/doc/stable/reference/index.html#reference","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"6a688c","input":"Linear regression naturally works best on highly correlated data, so I'm going to create a heatmap to see which variables are correlated! ","metadata":{"id":"J3L_hb0Ou4sn"},"pos":12,"type":"cell"}
{"cell_type":"markdown","id":"75bcb3","input":"Read in your data into a pandas dataframe by replacing the `filename` variable with your file's path. You can also use the current code below to work on a mpg dataset, where the target variable we are predicting is **miles per gallon** based on other car features. \n\n> We should choose two columns that we want to run regresssion on. Use the `.head()` function to decide which columns would be best!\n\n","metadata":{"id":"m4-2ySg9W0Fw"},"pos":3,"type":"cell"}
{"cell_type":"markdown","id":"834635","input":"### Results and Evaluation\n\nOne way to see if the model is pretty good is the coefficient of determination (R^2) using the `score()` function. You can read about it here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score.\n\nAnother way is to compare our mean absolute error (MAE). MAE measures the prediction error. Mathematically, it is the average absolute difference between observed and predicted outcomes, MAE = mean(abs(observeds - predicteds)). MAE is less sensitive to outliers compared to RMSE.\n\nRead some more about regression model metrics [here](http://www.sthda.com/english/articles/38-regression-model-validation/158-regression-model-accuracy-metrics-r-square-aic-bic-cp-and-more/).\n\n","metadata":{"id":"14TdY4RGr-RG"},"pos":27,"type":"cell"}
{"cell_type":"markdown","id":"84dcb8","input":"# Linear Regression with Scikit-learn's linear regression\n\nWe can use Scikit-Learnâ€™s Linear Regression to fit the model. Most other models we will use in the course \n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html","metadata":{"id":"cbeaQMpna-sR"},"pos":16,"type":"cell"}
{"cell_type":"markdown","id":"b38895","input":"### Creating Predictions\nPredict outputs on our x_test data that we held out. Think of this as a way to see how the model does on new data!","metadata":{"id":"QFb-_EiFrvzQ"},"pos":23,"type":"cell"}
{"cell_type":"markdown","id":"b8e1b5","input":"pandas docs - https://pandas.pydata.org/docs/reference/index.html#api <br>\nseaborn docs - https://seaborn.pydata.org/api.html","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"cfc7f8","input":"# Linear regression using SKLearn on your own Data!\nThis should look familiar... we now are going to use linear regression on some of our own features. I recommend walking through the code below first, then importing your dataset and working through the same problem with your data!","metadata":{"id":"K_X2fB9ta-rv"},"pos":0,"type":"cell"}
{"cell_type":"markdown","id":"ddf4cc","input":"# Repeat the process!\nTry running linear regression on multiple combinations of features (columns) on your dataset. What combination yields the best score? How does this connect to your correlation chart? ","metadata":{"id":"2OixzlOot-U6"},"pos":36,"type":"cell"}
{"cell_type":"markdown","id":"e4ea6f","input":"### Getting to know the problem\n\nFor my data, my columns inlude `'mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year', 'origin', and 'name'`. \n\nTo start, I would like to create a linear regression model that uses horsepower (X) to predict miles per gallon (y) and see how strong our linear regression model is. For your data, you should choose two columns as well to represent X and y.\n\n","metadata":{"id":"w78dUshNZarz"},"pos":14,"type":"cell"}
{"cell_type":"markdown","id":"ea1a2e","input":"### Split the data\n\nOur model should ignore 20% of data points to use for testing so it doesn't just memorize the data. We need to make sure there are no missing data points before continuing.\n\n","metadata":{"id":"g5Zk8_6ksLur"},"pos":17,"type":"cell"}
{"end":1658945940105,"id":"1bd826","input":"optimizer.best_params_","kernel":"python3-ubuntu","pos":22.75,"start":1658945940105,"state":"done","type":"cell"}
{"end":1658946057221,"exec_count":29,"id":"b0e82c","input":"import numpy as np\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\n\npoly_grid = GridSearchCV(PolynomialRegression(), , scoring='neg_mean_squared_error')\npoly_grid.fit(X,y)","kernel":"python3-ubuntu","output":{"0":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"1":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"10":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"11":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"12":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"13":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"14":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"15":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"16":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"17":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"18":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"19":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"2":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"20":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"21":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"22":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"23":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"24":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"25":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"26":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"27":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"28":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"29":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"3":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"30":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"31":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"32":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"33":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"34":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"35":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"36":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"37":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"38":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"39":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"4":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"40":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"41":{"ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-264e3aeecae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpoly_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolynomialRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpoly_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    716\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_residues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_residues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/linalg/_basic.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreal_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0mlwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_compute_lwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlapack_lwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m                 x, s, rank, info = lapack_func(a1, b1, lwork,\n\u001b[0m\u001b[1;32m   1205\u001b[0m                                                iwork, cond, False, False)\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# complex data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},"5":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"6":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"7":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"8":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"},"9":{"name":"stderr","text":"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_base.py:141: FutureWarning:\n\n'normalize' was deprecated in version 1.0 and will be removed in 1.2.\nIf you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n\nfrom sklearn.pipeline import make_pipeline\n\nmodel = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n\nIf you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n\nkwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\nmodel.fit(X, y, **kwargs)\n\n\n\n"}},"pos":22.5,"start":1658945985378,"state":"done","type":"cell"}
{"id":"f1ed12","input":"","pos":24.5,"type":"cell"}
{"id":0,"time":1658946065498,"type":"user"}
{"last_load":1658941283107,"type":"file"}